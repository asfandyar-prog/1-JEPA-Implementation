ğŸ§  Toy I-JEPA (Image Joint Embedding Predictive Architecture)

A minimal, from-scratch PyTorch implementation of a toy I-JEPA-style self-supervised vision model, inspired by Meta AIâ€™s Image Joint Embedding Predictive Architecture.

This repository is not a reproduction of the full I-JEPA paper.
Instead, it is designed to build deep intuition about the core ideas behind JEPA-style learning by implementing the smallest possible working system.

â¸»

ğŸ“Œ Motivation

Modern self-supervised learning methods often rely on:
	â€¢	heavy data augmentations
	â€¢	pixel-level reconstruction
	â€¢	contrastive objectives with negative samples

JEPA takes a different route:

Learn by predicting missing representations from visible context â€” without reconstructing pixels and without contrastive losses.

This project focuses on understanding that idea clearly and concretely, without abstraction-heavy frameworks or large-scale engineering.

â¸»

ğŸ” What This Project Implements

âœ” Patch-based Vision Transformer (ViT-style) encoder
âœ” Contextâ€“target masking over image patches
âœ” Studentâ€“teacher (EMA) architecture
âœ” Predictor that infers target representations from context representations
âœ” L2 loss in representation space (no reconstruction, no contrastive loss)
âœ” Visualization of context vs target patches on real images

â¸»

âŒ What This Project Does NOT Do
	â€¢	âŒ No large-scale pretraining
	â€¢	âŒ No multi-crop or heavy data augmentations
	â€¢	âŒ No classification head or downstream evaluation
	â€¢	âŒ No pixel reconstruction (not MAE)
	â€¢	âŒ No contrastive learning (not DINO / SimCLR)

This is an educational + research-intuition implementation.

â¸»

ğŸ§© High-Level Architecture

Input Image
   â”‚
   â”œâ”€ Patch Embedding (16Ã—16)
   â”‚
   â”œâ”€ Context Mask â”€â”€â–º Student Encoder â”€â”€â–º Context Representations
   â”‚
   â”œâ”€ Target Mask â”€â”€â”€â–º Teacher Encoder â”€â”€â–º Target Representations (EMA, no grad)
   â”‚
   â””â”€ Predictor
        â””â”€ Predict target representations from context


Training Objective

\mathcal{L} = \| \hat{z}_{target} - z_{target} \|_2^2

â¸»

ğŸ§ª Dataset
	â€¢	CIFAR-10, resized to 224 Ã— 224
	â€¢	Used only to test correctness and learning dynamics
	â€¢	No labels are used (fully self-supervised)

â¸»

ğŸ“Š Visualization

The code includes a visualization showing:
	â€¢	ğŸŸ© Green patches â†’ context (visible to student)
	â€¢	ğŸŸ¥ Red patches â†’ target (hidden, must be predicted)

This helps verify that:
	â€¢	the masking logic is correct
	â€¢	the learning task matches the JEPA formulation

â¸»

ğŸš€ How to Run
	1.	Open the notebook / script
	2.	Run top â†’ bottom
	3.	Verify:
	â€¢	loss decreases
	â€¢	visualization appears correctly
	â€¢	no runtime / autograd / device errors

The model is intentionally small and should run on:
	â€¢	CPU
	â€¢	Colab
	â€¢	Kaggle
	â€¢	Consumer GPUs

â¸»

ğŸ§  Key Takeaways
	â€¢	JEPA learns semantics, not pixels
	â€¢	Prediction happens in representation space
	â€¢	No contrastive negatives are required
	â€¢	The teacher provides a stable target via EMA
	â€¢	Masking defines what must be understood, not what must be reconstructed

â¸»

ğŸ“š References
	â€¢	LeCun et al., â€œSelf-Supervised Learning: The Dark Matter of Intelligenceâ€
	â€¢	Assran et al., â€œI-JEPA: Self-Supervised Learning from Images by Predicting Joint Embeddingsâ€
	â€¢	Vision Transformer (Dosovitskiy et al.)

â¸»

âš ï¸ Disclaimer

This code is not an official implementation of I-JEPA and should not be used for benchmarking or performance claims.

It exists purely to:

understand, reason about, and experiment with JEPA-style learning.

â¸»

âœ¨ Author Note

This project was built through iterative debugging and reasoning, not copy-paste.
If you are reading the code carefully and asking why each part exists â€” you are using it correctly.
